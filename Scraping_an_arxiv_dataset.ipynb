{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraping an arxiv dataset",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNaZdY9lEtoYjE391UJQWXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/depersisc/Python-scripts/blob/master/Scraping_an_arxiv_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djDp2XlODyco"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAJ7w_2CRdv8"
      },
      "source": [
        "#This code downloads the abstracts of all the papers published in Arxiv by MTI EECS faculty advisors, by using Beautifulsoup\n",
        "\n",
        "mit_url = \"https://www.eecs.mit.edu/people/faculty-advisors\"\n",
        "# Use requests to retrieve data from a given URL\n",
        "req = requests.get(mit_url)\n",
        "# Parse the whole HTML page using BeautifulSoup\n",
        "mit_soup = BeautifulSoup(req.content,features=\"html.parser\")\n",
        "\n",
        "#print(soup.prettify)\n",
        "\n",
        "field = mit_soup.find_all(class_ = \"field-content card-title\")\n",
        "\n",
        "names = []\n",
        "\n",
        "for i in field:\n",
        "  names.append(i.get_text())\n",
        "\n",
        "\n",
        "string = \"https://arxiv.org/search/?query=\"\n",
        "\n",
        "string_2 = \"&searchtype=all&source=header\" \n",
        "\n",
        "urls = []\n",
        "\n",
        "for i in names:\n",
        "  urls.append(string+i+string_2) #build the url to search for the author name i in arxiv\n",
        "  \n",
        "\n",
        "for idx_1,url in enumerate(urls[0:10]):  #enumerate(urls)= (1, url search first author), (2, urls search 2nd author)..)\n",
        "  \n",
        "  req = requests.get(url)\n",
        "  \n",
        "  soup = BeautifulSoup(req.content,\"html.parser\")  \n",
        "  \n",
        "  field = soup.find_all(class_=\"abstract-full\")\n",
        "  \n",
        "  for idx_2,val in enumerate(field):\n",
        "    text = val.get_text() #memorize the abstract in text\n",
        "    name = \"abstract_%s_%s.txt\" %(names[idx_1],idx_2+1)\n",
        "    with open(name, 'wb') as f:\n",
        "      f.write(text.encode(\"utf-8\"))\n",
        "    files.download(name)\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}